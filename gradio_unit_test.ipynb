{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rnltl\\anaconda3\\envs\\gemma\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from urllib import request\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import gradio as gr\n",
    "from gradio import ChatMessage\n",
    "from PIL import Image, ImageOps\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "c:\\Users\\rnltl\\anaconda3\\envs\\gemma\\lib\\site-packages\\transformers\\quantizers\\auto.py:174: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "class HPChatBot:\n",
    "    def __init__(self,\n",
    "                 model_path: str = 'rnltls/harrypotter_lexicon_finetune',\n",
    "                 device_map: str = 'auto',\n",
    "                 load_in_4_bit: bool = True,\n",
    "                 **quant_kwargs) -> None:\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.image_processor = None\n",
    "        self.conv = None\n",
    "        self.conv_img = []\n",
    "        self.img_tensor = []\n",
    "        self.roles = None\n",
    "        self.stop_key = None\n",
    "        self.is_chat = False\n",
    "        self.is_waldo = False\n",
    "        self.load_models(model_path,\n",
    "                         device_map=device_map,\n",
    "                         load_in_4_bit=load_in_4_bit,\n",
    "                         **quant_kwargs)\n",
    "\n",
    "    def load_models(self, model_path: str,\n",
    "                    device_map: str,\n",
    "                    load_in_4_bit: bool,\n",
    "                    **quant_kwargs) -> None:\n",
    "        model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "            \"rnltls/harrypotter_lexicon_finetune\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "            load_in_4bit = load_in_4_bit,\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"rnltls/harrypotter_lexicon_finetune\")\n",
    "\n",
    "        self.model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "            \"rnltls/harrypotter_lexicon_finetune\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "            load_in_4bit = load_in_4_bit,\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path,\n",
    "                                                       use_fast=False)\n",
    "\n",
    "    # def setup_image(self, img_path: str) -> None:\n",
    "    #     \"\"\"Load and process the image.\"\"\"\n",
    "    #     if img_path.startswith('http') or img_path.startswith('https'):\n",
    "    #         response = requests.get(img_path)\n",
    "    #         self.conv_img = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    #     else:\n",
    "    #         self.conv_img = Image.open(img_path).convert('RGB')\n",
    "    #     self.img_tensor = self.image_processor.preprocess(self.conv_img,\n",
    "    #                                                       return_tensors='pt'\n",
    "    #                                                       )['pixel_values'].half().cuda()\n",
    "\n",
    "    # def set_image(self, img) -> None:\n",
    "    #     self.conv_img.append(img)\n",
    "    #     self.img_tensor.append(self.image_processor.preprocess(self.conv_img[-1],\n",
    "    #                                                       return_tensors='pt'\n",
    "    #                                                       )['pixel_values'].half().cuda())\n",
    "\n",
    "    # def replace_image(self, img) -> None:\n",
    "    #     self.conv_img[-1] = img\n",
    "    #     self.img_tensor[-1] = self.image_processor.preprocess(self.conv_img[-1],\n",
    "    #                                                       return_tensors='pt'\n",
    "    #                                                       )['pixel_values'].half().cuda()\n",
    "        \n",
    "    # def generate_answer(self, **kwargs) -> str:\n",
    "    #     \"\"\"Generate an answer from the current conversation.\"\"\"\n",
    "    #     raw_prompt = self.conv.get_prompt()\n",
    "        \n",
    "    #     input_ids = tokenizer_image_token(raw_prompt,\n",
    "    #                                       self.tokenizer,\n",
    "    #                                       IMAGE_TOKEN_INDEX,\n",
    "    #                                       return_tensors='pt').unsqueeze(0).cuda()\n",
    "    #     stopping = KeywordsStoppingCriteria([self.stop_key],\n",
    "    #                                         self.tokenizer,\n",
    "    #                                         input_ids)\n",
    "    #     with torch.inference_mode():\n",
    "    #         if self.img_tensor is not []:\n",
    "    #             output_ids = self.model.generate(input_ids,\n",
    "    #                                             images=self.img_tensor,\n",
    "    #                                             stopping_criteria=[stopping],\n",
    "    #                                             **kwargs)\n",
    "    #     outputs = self.tokenizer.decode(\n",
    "    #         output_ids[0, input_ids.shape[1]:]\n",
    "    #     ).strip()\n",
    "    #     self.conv.messages[-1][-1] = outputs\n",
    "\n",
    "    #     return outputs.rsplit('</s>', 1)[0]\n",
    "\n",
    "    # def get_conv_text(self) -> str:\n",
    "    #     \"\"\"Return full conversation text.\"\"\"\n",
    "    #     return self.conv.get_prompt()\n",
    "    \n",
    "    # def set_conv(self, conv):\n",
    "    #     self.conv = conv\n",
    "    \n",
    "    # def get_conv(self):\n",
    "    #     return self.conv\n",
    "    \n",
    "    # def start_new_chat(self,\n",
    "    #                    do_sample=True,\n",
    "    #                    temperature=0.2,\n",
    "    #                    max_new_tokens=1024,\n",
    "    #                    use_cache=True,\n",
    "    #                    **kwargs) -> str:\n",
    "    #     \"\"\"Start a new chat with a new image.\"\"\"\n",
    "        \n",
    "    #     self.roles = self.conv.roles\n",
    "    #     if self.conv.sep_style == SeparatorStyle.TWO:\n",
    "    #         self.stop_key = self.conv.sep2\n",
    "    #     else:\n",
    "    #         self.stop_key = self.conv.sep\n",
    "            \n",
    "    #     answer = self.generate_answer(do_sample=do_sample,\n",
    "    #                                   temperature=temperature,\n",
    "    #                                   max_new_tokens=max_new_tokens,\n",
    "    #                                   use_cache=use_cache,\n",
    "    #                                   **kwargs)\n",
    "    #     return answer, self.conv\n",
    "    \n",
    "    # def continue_chat(self,\n",
    "    #                   do_sample=True,\n",
    "    #                   temperature=0.2,\n",
    "    #                   max_new_tokens=1024,\n",
    "    #                   use_cache=True,\n",
    "    #                   **kwargs) -> str:\n",
    "    #     \"\"\"Continue the existing chat.\"\"\"\n",
    "    #     if self.conv is None:\n",
    "    #         raise RuntimeError(\"No existing conversation found. Start a new\"\n",
    "    #                            \"conversation using the `start_new_chat` method.\")\n",
    "    #     answer = self.generate_answer(do_sample=do_sample,\n",
    "    #                                   temperature=temperature,\n",
    "    #                                   max_new_tokens=max_new_tokens,\n",
    "    #                                   use_cache=use_cache,\n",
    "    #                                   **kwargs)\n",
    "    #     return answer\n",
    "    \n",
    "    # def split_image(self, tile_size = 336):\n",
    "    #     img = self.conv_img[-1]\n",
    "        \n",
    "    #     width, height = img.size\n",
    "    #     # 가로, 세로로 패딩이 필요한지 계산\n",
    "    #     pad_width = (tile_size - width % tile_size) % tile_size\n",
    "    #     pad_height = (tile_size - height % tile_size) % tile_size\n",
    "\n",
    "    #     # 좌우 및 상하 패딩 계산\n",
    "    #     left_pad = pad_width // 2\n",
    "    #     right_pad = pad_width - left_pad\n",
    "    #     top_pad = pad_height // 2\n",
    "    #     bottom_pad = pad_height - top_pad\n",
    "\n",
    "    #     # 패딩 추가\n",
    "    #     img_padded = ImageOps.expand(img, (left_pad, top_pad, right_pad, bottom_pad), fill='white')\n",
    "\n",
    "    #     # 패딩된 이미지의 크기\n",
    "    #     padded_width, padded_height = img_padded.size\n",
    "        \n",
    "    #     tiles = []\n",
    "    #     for y in range(0, padded_height, tile_size):\n",
    "    #         for x in range(0, padded_width, tile_size):\n",
    "    #             tiles.append(img_padded.crop((x, y, x + tile_size, y + tile_size)))\n",
    "\n",
    "    #     return tiles\n",
    "    \n",
    "    # def find_waldo(self,\n",
    "    #                   do_sample=True,\n",
    "    #                   temperature=0.2,\n",
    "    #                   max_new_tokens=1024,\n",
    "    #                   use_cache=True,\n",
    "    #                   **kwargs) -> str:\n",
    "    #     if self.conv is None:\n",
    "    #         raise RuntimeError(\"No existing conversation found. Start a new\"\n",
    "    #                            \"conversation using the `start_new_chat` method.\")\n",
    "        \n",
    "    #     # img를 나눈다.\n",
    "    #     splitted_imgs = self.split_image()\n",
    "    #     # 나눈 이미지를 loop를 돌린다.\n",
    "    #     # 동일한 질의를 진행하며, Yes라고 나온 이미지를 저장한다.\n",
    "    #     # 직전의 질의를 지우고, Yes라고 나온 이미지에서 Waldo가 무엇을 하고있는지 질의로 변경하고, 모든 이미지에 대해서 수행하여 state에 등록한다.\n",
    "    #     text, origin_img, image_process_mode = self.conv.messages[-2][-1]\n",
    "\n",
    "    #     state = self.conv\n",
    "    #     answer_list = []\n",
    "    #     text = \"Find Waldo. if not visible, dont Answer what Waldo is doing. if visible, Answer what Waldo is doing.\"\n",
    "    #     for idx, choosen_img in enumerate(splitted_imgs):\n",
    "    #         self.conv.messages[-2][-1] = (text, choosen_img, image_process_mode)\n",
    "    #         self.replace_image(choosen_img)\n",
    "    #         answer = self.generate_answer(do_sample=do_sample,\n",
    "    #                                   temperature=temperature,\n",
    "    #                                   max_new_tokens=max_new_tokens,\n",
    "    #                                   use_cache=use_cache,\n",
    "    #                                   **kwargs)\n",
    "    #         answer_list.append(answer)\n",
    "    #         self.conv.messages[-1][-1] = None\n",
    "    #         print(f\"Answer Split IMG {idx+1}\")\n",
    "\n",
    "    #     self.conv.messages[-2][-1] = (\"Where's Waldo?\", origin_img, image_process_mode)\n",
    "    #     self.conv.messages[-1][-1] = \"Picture that Waldo is in it and what he's doing.\"\n",
    "\n",
    "    #     for answer, choosen_img in zip(answer_list, splitted_imgs):\n",
    "    #         state.append_message(state.roles[0], (\"\", choosen_img, image_process_mode))\n",
    "    #         state.append_message(state.roles[1], (answer))\n",
    "    #         self.set_image(choosen_img)\n",
    "    #         self.conv_img.append(choosen_img)\n",
    "            \n",
    "    #     self.conv = state\n",
    "    #     return answer\n",
    "    \n",
    "    # def set_chat_type(self, chat_type):\n",
    "    #     chat_type = chat_type.lower()\n",
    "    #     if chat_type == 'chat':\n",
    "    #         self.is_chat = True\n",
    "    #     elif chat_type == 'waldo':\n",
    "    #         self.is_waldo = True\n",
    "\n",
    "    # def clear(self):\n",
    "    #     self.conv = None\n",
    "    #     self.img_tensor = []\n",
    "    #     self.conv_img = []\n",
    "    #     self.is_chat = False\n",
    "    #     self.is_waldo = False\n",
    "\n",
    "HP_chatbot = HPChatBot(load_in_8bit=True,\n",
    "                bnb_8bit_compute_dtype=torch.float16,\n",
    "                bnb_8bit_use_double_quant=True,\n",
    "                bnb_8bit_quant_type='nf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://848ba3ef089029795e.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://848ba3ef089029795e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enable_btn = gr.Button(interactive=True)\n",
    "disable_btn = gr.Button(interactive=False)\n",
    "spell_chat, potion_chat, other_chat, house_chat = False, False, False, False\n",
    "\n",
    "question_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "def show_image(path):\n",
    "    # Convert To PIL Image\n",
    "    image = Image.open(path)\n",
    "    return image\n",
    "\n",
    "def spell_chatting(chat_history, request: gr.Request):\n",
    "    global spell_chat, potion_chat, other_chat, house_chat\n",
    "    chat_history.clear()\n",
    "    print(f\"chatting ip: {request.client.host}\")\n",
    "    spell_chat, potion_chat, other_chat, house_chat = True, False, False, False\n",
    "    bot_message = \"Welcome to Magic Spell Class!\\nTell me what you want to achieve, and I’ll suggest the perfect spell for it!\\nIf you ask in the format: 'What spell can I use when I ~?', I can give you even better suggestions!\"\n",
    "    txt_box = gr.Textbox(value=\"What spell can I use when I \", interactive=True)\n",
    "    prof_IMG = show_image(\"./IMG/Spell_stand.jpg\")\n",
    "    chat_history.append([None,bot_message])\n",
    "    return chat_history, prof_IMG, txt_box, enable_btn, enable_btn, enable_btn\n",
    "\n",
    "def potion_chatting(chat_history, request: gr.Request):\n",
    "    global spell_chat, potion_chat, other_chat, house_chat\n",
    "    chat_history.clear()\n",
    "    print(f\"chatting ip: {request.client.host}\")\n",
    "    spell_chat, potion_chat, other_chat, house_chat = False, True, False, False\n",
    "    bot_message = \"Welcome to Potion Class!\\nTell me what you want to achieve, and I’ll suggest the perfect potion for it!\\nIf you ask in the format: 'What potion can I make when I ~?', I can give you even better suggestions!\"\n",
    "    txt_box = gr.Textbox(value=\"What potion can I make when I \", interactive=True)\n",
    "    prof_IMG = show_image(\"./IMG/Potion_stand.jpg\")\n",
    "    chat_history.append([None,bot_message])\n",
    "    return chat_history, prof_IMG, txt_box, enable_btn, enable_btn, enable_btn\n",
    "\n",
    "def other_chatting(chat_history, request: gr.Request):\n",
    "    global spell_chat, potion_chat, other_chat, house_chat\n",
    "    chat_history.clear()\n",
    "    print(f\"chatting ip: {request.client.host}\")\n",
    "    spell_chat, potion_chat, other_chat, house_chat = False, False, True, False\n",
    "    bot_message = \"Welcome to Library!\\nWelcome to the library! Feel free to ask me anything if you're curious!\"\n",
    "    txt_box = gr.Textbox(placeholder=\"Ask anything you're curious about in the Wizarding World!\", value=\"\", interactive=True)\n",
    "    prof_IMG = show_image(\"./IMG/Other_stand.jpg\")\n",
    "    chat_history.append([None,bot_message])\n",
    "    return chat_history, prof_IMG, txt_box, enable_btn, enable_btn, enable_btn\n",
    "\n",
    "def house_chatting(chat_history, request: gr.Request):\n",
    "    global spell_chat, potion_chat, other_chat, house_chat\n",
    "    chat_history.clear()\n",
    "    print(f\"chatting ip: {request.client.host}\")\n",
    "    spell_chat, potion_chat, other_chat, house_chat = False, False, False, True\n",
    "    bot_message = \"Hmm, let's see… where shall I place you?\"\n",
    "    txt_box = gr.Textbox(placeholder=\"faiosdjfopasdjfoasdjfopsd\", value=\"\", interactive=True)\n",
    "    prof_IMG = show_image(\"./IMG/sorting_hat.jpg\")\n",
    "    chat_history.append([None,bot_message])\n",
    "    return chat_history, prof_IMG, txt_box, enable_btn, enable_btn, enable_btn\n",
    "\n",
    "def ask_question(chat_history, text_data, request: gr.Request):\n",
    "    global spell_chat, potion_chat, other_chat, house_chat\n",
    "    print(spell_chat, potion_chat, other_chat, house_chat)\n",
    "    if spell_chat:\n",
    "        print(text_data)\n",
    "        prof_IMG = show_image(\"./IMG/Spell_think.jpg\")\n",
    "        chat_history.append([text_data, None])\n",
    "        text_data = gr.Textbox(value=\"What spell can I use when I \", interactive=True)\n",
    "        return chat_history, prof_IMG, text_data\n",
    "    \n",
    "    if potion_chat:\n",
    "        print(text_data)\n",
    "        prof_IMG = show_image(\"./IMG/Potion_think.jpg\")\n",
    "        chat_history.append([text_data, None])\n",
    "        text_data = gr.Textbox(placeholder=\"Ask anything you're curious about in the Wizarding World!\", value=\"What potion can I make when I \", interactive=True)\n",
    "        return chat_history, prof_IMG, text_data\n",
    "    \n",
    "    if other_chat:\n",
    "        print(text_data)\n",
    "        prof_IMG = show_image(\"./IMG/Other_think.jpg\")\n",
    "        chat_history.append([\"test message\", None])\n",
    "        text_data = gr.Textbox(placeholder=\"faiosdjfopasdjfoasdjfopsd\", value=\"\", interactive=True)\n",
    "        return chat_history, prof_IMG, text_data\n",
    "    \n",
    "    if house_chat:\n",
    "        print(text_data)\n",
    "        prof_IMG = show_image(\"./IMG/sorting_hat.jpg\")\n",
    "        chat_history.append([\"test message\", None])\n",
    "        text_data = gr.Textbox(value=\"asdfasdfasdfasdf\", interactive=True)\n",
    "        return chat_history, prof_IMG, text_data\n",
    "\n",
    "background_image_path = Path(\"./IMG/bg1.jpg\").resolve()\n",
    "\n",
    "\n",
    "\n",
    "custom_css = f\"\"\"\n",
    "\n",
    "body {{\n",
    "    background-image: url('file://{background_image_path}');\n",
    "    background-size: cover;  /* 배경 이미지가 화면 전체를 덮도록 설정 */\n",
    "    background-repeat: no-repeat;  /* 이미지 반복 방지 */\n",
    "    background-position: center;  /* 이미지 가운데 정렬 */\n",
    "}}\n",
    "\"\"\"\n",
    "custom_css = f\"\"\"\n",
    "body {{\n",
    "    background-image: url('file://{background_image_path}');\n",
    "    background-size: cover;  /* Make the image cover the entire background */\n",
    "    background-position: center;  /* Center the background image */\n",
    "    background-repeat: no-repeat;  /* Do not repeat the background image */\n",
    "}}\n",
    "\n",
    ".dark body {{\n",
    "    background-image: url('file://{background_image_path}');\n",
    "    background-size: cover;\n",
    "    background-position: center;\n",
    "    background-repeat: no-repeat;\n",
    "    background-color: #333;  /* Optional: Fallback color for dark mode */\n",
    "}}\n",
    "\n",
    ".gradio-container {{\n",
    "    background: transparent !important;  /* Make container background transparent to see the full background image */\n",
    "}}\n",
    "\"\"\"\n",
    "def build_gradio(concurrency_count=10):\n",
    "    textbox = gr.Textbox(show_label=False, placeholder=\"Welcome to Hogwart!\", container=False, interactive = False)\n",
    "    with gr.Blocks(\n",
    "            css=custom_css,\n",
    "            theme='abidlabs/pakistan',\n",
    "            \n",
    "            \n",
    "        ) as demo:\n",
    "        state = gr.State()\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=3):\n",
    "\n",
    "                # imagebox = gr.Image(interactive=False)\n",
    "                imagebox = gr.Image(value=show_image(\"./IMG/HogwartGemma.jpg\"), type=\"pil\", interactive=False, show_label=False, show_download_button=False, show_fullscreen_button=False)\n",
    "                # image_process_mode = gr.Radio(\n",
    "                #     [\"Crop\", \"Resize\", \"Pad\", \"Default\"],\n",
    "                #     value=\"Default\",\n",
    "                #     label=\"Preprocess for non-square image\", visible=False)\n",
    "                \n",
    "                spell_btn = gr.Button(value=\"🪄 Spell\", interactive = True, scale = 2)\n",
    "                potion_btn = gr.Button(value=\"🧉 Potion\", interactive = True, scale = 2)\n",
    "                other_btn = gr.Button(value=\"📚 Others\", interactive = True, scale = 2)\n",
    "                house_btn = gr.Button(value=\"🌟 Find your House\", interactive = True, scale = 2)\n",
    "\n",
    "            with gr.Column(scale=8):\n",
    "                chatbot = gr.Chatbot(\n",
    "                    label='Free to ask!',\n",
    "                    height=600,\n",
    "                )\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=8):\n",
    "                        textbox.render()\n",
    "                    with gr.Column(scale=1, min_width=50):\n",
    "                        submit_btn = gr.Button(value=\"Send\", variant=\"primary\", interactive = False)\n",
    "\n",
    "                with gr.Row(elem_id=\"buttons\") as button_row:\n",
    "                    finish_btn = gr.Button(value=\"🏁 End Talking\", interactive = False)\n",
    "                    clear_btn = gr.Button(value=\"🗑️  Clear\", interactive=False)    \n",
    "        box_list = [spell_btn, potion_btn, other_btn, house_btn, textbox, submit_btn, finish_btn, clear_btn]\n",
    "        spell_btn.click(\n",
    "            spell_chatting,\n",
    "            inputs=[chatbot],\n",
    "            outputs=[chatbot, imagebox, textbox, submit_btn, finish_btn, clear_btn]\n",
    "        )\n",
    "        potion_btn.click(\n",
    "            potion_chatting,\n",
    "            inputs=[chatbot],\n",
    "            outputs=[chatbot, imagebox, textbox, submit_btn, finish_btn, clear_btn]\n",
    "        )\n",
    "        other_btn.click(\n",
    "            other_chatting,\n",
    "            inputs=[chatbot],\n",
    "            outputs=[chatbot, imagebox, textbox, submit_btn, finish_btn, clear_btn]\n",
    "        )\n",
    "        house_btn.click(\n",
    "            house_chatting,\n",
    "            inputs=[chatbot],\n",
    "            outputs=[chatbot, imagebox, textbox, submit_btn, finish_btn, clear_btn]\n",
    "        )\n",
    "        submit_btn.click(\n",
    "            ask_question,\n",
    "            inputs = [chatbot, textbox],\n",
    "            outputs = [chatbot, imagebox, textbox]\n",
    "        )\n",
    "        finish_btn.click()\n",
    "        clear_btn.click()\n",
    "\n",
    "    return demo\n",
    "\n",
    "import argparse\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # 컨트롤러에서 사용가능한 모델 가져옴. 나는 모델 하나만 쓸 것이기 때문에, get_model_list()에서 해당 모델이 동작하고 있는 url을 넘겨주면 된다!\n",
    "    # models = [args.model_url] \n",
    "    \n",
    "    # HP_chatbot = HPChatBot(load_in_8bit=True,\n",
    "    #                    bnb_8bit_compute_dtype=torch.float16,\n",
    "    #                    bnb_8bit_use_double_quant=True,\n",
    "    #                    bnb_8bit_quant_type='nf8')\n",
    "    \n",
    "    # Gradio를 이용해서 데모 만들기\n",
    "    demo = build_gradio()\n",
    "    demo.queue(\n",
    "        api_open=False\n",
    "    ).launch(\n",
    "        server_port=7860,\n",
    "        share=True\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
