{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rnltl\\anaconda3\\envs\\gemma\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from urllib import request\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import gradio as gr\n",
    "from gradio import ChatMessage\n",
    "from PIL import Image, ImageOps\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "c:\\Users\\rnltl\\anaconda3\\envs\\gemma\\lib\\site-packages\\transformers\\quantizers\\auto.py:174: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "class HPChatBot:\n",
    "    def __init__(self,\n",
    "                 model_path: str = 'rnltls/harrypotter_lexicon_finetune',\n",
    "                 device_map: str = 'auto',\n",
    "                 load_in_4_bit: bool = True,\n",
    "                 **quant_kwargs) -> None:\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.image_processor = None\n",
    "        self.conv = None\n",
    "        self.conv_img = []\n",
    "        self.img_tensor = []\n",
    "        self.roles = None\n",
    "        self.stop_key = None\n",
    "        self.is_chat = False\n",
    "        self.is_waldo = False\n",
    "        self.load_models(model_path,\n",
    "                         device_map=device_map,\n",
    "                         load_in_4_bit=load_in_4_bit,\n",
    "                         **quant_kwargs)\n",
    "\n",
    "    def load_models(self, model_path: str,\n",
    "                    device_map: str,\n",
    "                    load_in_4_bit: bool,\n",
    "                    **quant_kwargs) -> None:\n",
    "        model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "            \"rnltls/harrypotter_lexicon_finetune\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "            load_in_4bit = load_in_4_bit,\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"rnltls/harrypotter_lexicon_finetune\")\n",
    "\n",
    "        self.model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "            \"rnltls/harrypotter_lexicon_finetune\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "            load_in_4bit = load_in_4_bit,\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path,\n",
    "                                                       use_fast=False)\n",
    "\n",
    "    # def setup_image(self, img_path: str) -> None:\n",
    "    #     \"\"\"Load and process the image.\"\"\"\n",
    "    #     if img_path.startswith('http') or img_path.startswith('https'):\n",
    "    #         response = requests.get(img_path)\n",
    "    #         self.conv_img = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    #     else:\n",
    "    #         self.conv_img = Image.open(img_path).convert('RGB')\n",
    "    #     self.img_tensor = self.image_processor.preprocess(self.conv_img,\n",
    "    #                                                       return_tensors='pt'\n",
    "    #                                                       )['pixel_values'].half().cuda()\n",
    "\n",
    "    # def set_image(self, img) -> None:\n",
    "    #     self.conv_img.append(img)\n",
    "    #     self.img_tensor.append(self.image_processor.preprocess(self.conv_img[-1],\n",
    "    #                                                       return_tensors='pt'\n",
    "    #                                                       )['pixel_values'].half().cuda())\n",
    "\n",
    "    # def replace_image(self, img) -> None:\n",
    "    #     self.conv_img[-1] = img\n",
    "    #     self.img_tensor[-1] = self.image_processor.preprocess(self.conv_img[-1],\n",
    "    #                                                       return_tensors='pt'\n",
    "    #                                                       )['pixel_values'].half().cuda()\n",
    "        \n",
    "    # def generate_answer(self, **kwargs) -> str:\n",
    "    #     \"\"\"Generate an answer from the current conversation.\"\"\"\n",
    "    #     raw_prompt = self.conv.get_prompt()\n",
    "        \n",
    "    #     input_ids = tokenizer_image_token(raw_prompt,\n",
    "    #                                       self.tokenizer,\n",
    "    #                                       IMAGE_TOKEN_INDEX,\n",
    "    #                                       return_tensors='pt').unsqueeze(0).cuda()\n",
    "    #     stopping = KeywordsStoppingCriteria([self.stop_key],\n",
    "    #                                         self.tokenizer,\n",
    "    #                                         input_ids)\n",
    "    #     with torch.inference_mode():\n",
    "    #         if self.img_tensor is not []:\n",
    "    #             output_ids = self.model.generate(input_ids,\n",
    "    #                                             images=self.img_tensor,\n",
    "    #                                             stopping_criteria=[stopping],\n",
    "    #                                             **kwargs)\n",
    "    #     outputs = self.tokenizer.decode(\n",
    "    #         output_ids[0, input_ids.shape[1]:]\n",
    "    #     ).strip()\n",
    "    #     self.conv.messages[-1][-1] = outputs\n",
    "\n",
    "    #     return outputs.rsplit('</s>', 1)[0]\n",
    "\n",
    "    # def get_conv_text(self) -> str:\n",
    "    #     \"\"\"Return full conversation text.\"\"\"\n",
    "    #     return self.conv.get_prompt()\n",
    "    \n",
    "    # def set_conv(self, conv):\n",
    "    #     self.conv = conv\n",
    "    \n",
    "    # def get_conv(self):\n",
    "    #     return self.conv\n",
    "    \n",
    "    # def start_new_chat(self,\n",
    "    #                    do_sample=True,\n",
    "    #                    temperature=0.2,\n",
    "    #                    max_new_tokens=1024,\n",
    "    #                    use_cache=True,\n",
    "    #                    **kwargs) -> str:\n",
    "    #     \"\"\"Start a new chat with a new image.\"\"\"\n",
    "        \n",
    "    #     self.roles = self.conv.roles\n",
    "    #     if self.conv.sep_style == SeparatorStyle.TWO:\n",
    "    #         self.stop_key = self.conv.sep2\n",
    "    #     else:\n",
    "    #         self.stop_key = self.conv.sep\n",
    "            \n",
    "    #     answer = self.generate_answer(do_sample=do_sample,\n",
    "    #                                   temperature=temperature,\n",
    "    #                                   max_new_tokens=max_new_tokens,\n",
    "    #                                   use_cache=use_cache,\n",
    "    #                                   **kwargs)\n",
    "    #     return answer, self.conv\n",
    "    \n",
    "    # def continue_chat(self,\n",
    "    #                   do_sample=True,\n",
    "    #                   temperature=0.2,\n",
    "    #                   max_new_tokens=1024,\n",
    "    #                   use_cache=True,\n",
    "    #                   **kwargs) -> str:\n",
    "    #     \"\"\"Continue the existing chat.\"\"\"\n",
    "    #     if self.conv is None:\n",
    "    #         raise RuntimeError(\"No existing conversation found. Start a new\"\n",
    "    #                            \"conversation using the `start_new_chat` method.\")\n",
    "    #     answer = self.generate_answer(do_sample=do_sample,\n",
    "    #                                   temperature=temperature,\n",
    "    #                                   max_new_tokens=max_new_tokens,\n",
    "    #                                   use_cache=use_cache,\n",
    "    #                                   **kwargs)\n",
    "    #     return answer\n",
    "    \n",
    "    # def split_image(self, tile_size = 336):\n",
    "    #     img = self.conv_img[-1]\n",
    "        \n",
    "    #     width, height = img.size\n",
    "    #     # ê°€ë¡œ, ì„¸ë¡œë¡œ íŒ¨ë”©ì´ í•„ìš”í•œì§€ ê³„ì‚°\n",
    "    #     pad_width = (tile_size - width % tile_size) % tile_size\n",
    "    #     pad_height = (tile_size - height % tile_size) % tile_size\n",
    "\n",
    "    #     # ì¢Œìš° ë° ìƒí•˜ íŒ¨ë”© ê³„ì‚°\n",
    "    #     left_pad = pad_width // 2\n",
    "    #     right_pad = pad_width - left_pad\n",
    "    #     top_pad = pad_height // 2\n",
    "    #     bottom_pad = pad_height - top_pad\n",
    "\n",
    "    #     # íŒ¨ë”© ì¶”ê°€\n",
    "    #     img_padded = ImageOps.expand(img, (left_pad, top_pad, right_pad, bottom_pad), fill='white')\n",
    "\n",
    "    #     # íŒ¨ë”©ëœ ì´ë¯¸ì§€ì˜ í¬ê¸°\n",
    "    #     padded_width, padded_height = img_padded.size\n",
    "        \n",
    "    #     tiles = []\n",
    "    #     for y in range(0, padded_height, tile_size):\n",
    "    #         for x in range(0, padded_width, tile_size):\n",
    "    #             tiles.append(img_padded.crop((x, y, x + tile_size, y + tile_size)))\n",
    "\n",
    "    #     return tiles\n",
    "    \n",
    "    # def find_waldo(self,\n",
    "    #                   do_sample=True,\n",
    "    #                   temperature=0.2,\n",
    "    #                   max_new_tokens=1024,\n",
    "    #                   use_cache=True,\n",
    "    #                   **kwargs) -> str:\n",
    "    #     if self.conv is None:\n",
    "    #         raise RuntimeError(\"No existing conversation found. Start a new\"\n",
    "    #                            \"conversation using the `start_new_chat` method.\")\n",
    "        \n",
    "    #     # imgë¥¼ ë‚˜ëˆˆë‹¤.\n",
    "    #     splitted_imgs = self.split_image()\n",
    "    #     # ë‚˜ëˆˆ ì´ë¯¸ì§€ë¥¼ loopë¥¼ ëŒë¦°ë‹¤.\n",
    "    #     # ë™ì¼í•œ ì§ˆì˜ë¥¼ ì§„í–‰í•˜ë©°, Yesë¼ê³  ë‚˜ì˜¨ ì´ë¯¸ì§€ë¥¼ ì €ì¥í•œë‹¤.\n",
    "    #     # ì§ì „ì˜ ì§ˆì˜ë¥¼ ì§€ìš°ê³ , Yesë¼ê³  ë‚˜ì˜¨ ì´ë¯¸ì§€ì—ì„œ Waldoê°€ ë¬´ì—‡ì„ í•˜ê³ ìˆëŠ”ì§€ ì§ˆì˜ë¡œ ë³€ê²½í•˜ê³ , ëª¨ë“  ì´ë¯¸ì§€ì— ëŒ€í•´ì„œ ìˆ˜í–‰í•˜ì—¬ stateì— ë“±ë¡í•œë‹¤.\n",
    "    #     text, origin_img, image_process_mode = self.conv.messages[-2][-1]\n",
    "\n",
    "    #     state = self.conv\n",
    "    #     answer_list = []\n",
    "    #     text = \"Find Waldo. if not visible, dont Answer what Waldo is doing. if visible, Answer what Waldo is doing.\"\n",
    "    #     for idx, choosen_img in enumerate(splitted_imgs):\n",
    "    #         self.conv.messages[-2][-1] = (text, choosen_img, image_process_mode)\n",
    "    #         self.replace_image(choosen_img)\n",
    "    #         answer = self.generate_answer(do_sample=do_sample,\n",
    "    #                                   temperature=temperature,\n",
    "    #                                   max_new_tokens=max_new_tokens,\n",
    "    #                                   use_cache=use_cache,\n",
    "    #                                   **kwargs)\n",
    "    #         answer_list.append(answer)\n",
    "    #         self.conv.messages[-1][-1] = None\n",
    "    #         print(f\"Answer Split IMG {idx+1}\")\n",
    "\n",
    "    #     self.conv.messages[-2][-1] = (\"Where's Waldo?\", origin_img, image_process_mode)\n",
    "    #     self.conv.messages[-1][-1] = \"Picture that Waldo is in it and what he's doing.\"\n",
    "\n",
    "    #     for answer, choosen_img in zip(answer_list, splitted_imgs):\n",
    "    #         state.append_message(state.roles[0], (\"\", choosen_img, image_process_mode))\n",
    "    #         state.append_message(state.roles[1], (answer))\n",
    "    #         self.set_image(choosen_img)\n",
    "    #         self.conv_img.append(choosen_img)\n",
    "            \n",
    "    #     self.conv = state\n",
    "    #     return answer\n",
    "    \n",
    "    # def set_chat_type(self, chat_type):\n",
    "    #     chat_type = chat_type.lower()\n",
    "    #     if chat_type == 'chat':\n",
    "    #         self.is_chat = True\n",
    "    #     elif chat_type == 'waldo':\n",
    "    #         self.is_waldo = True\n",
    "\n",
    "    # def clear(self):\n",
    "    #     self.conv = None\n",
    "    #     self.img_tensor = []\n",
    "    #     self.conv_img = []\n",
    "    #     self.is_chat = False\n",
    "    #     self.is_waldo = False\n",
    "\n",
    "HP_chatbot = HPChatBot(load_in_8bit=True,\n",
    "                bnb_8bit_compute_dtype=torch.float16,\n",
    "                bnb_8bit_use_double_quant=True,\n",
    "                bnb_8bit_quant_type='nf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://848ba3ef089029795e.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://848ba3ef089029795e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enable_btn = gr.Button(interactive=True)\n",
    "disable_btn = gr.Button(interactive=False)\n",
    "spell_chat, potion_chat, other_chat, house_chat = False, False, False, False\n",
    "\n",
    "question_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "def show_image(path):\n",
    "    # Convert To PIL Image\n",
    "    image = Image.open(path)\n",
    "    return image\n",
    "\n",
    "def spell_chatting(chat_history, request: gr.Request):\n",
    "    global spell_chat, potion_chat, other_chat, house_chat\n",
    "    chat_history.clear()\n",
    "    print(f\"chatting ip: {request.client.host}\")\n",
    "    spell_chat, potion_chat, other_chat, house_chat = True, False, False, False\n",
    "    bot_message = \"Welcome to Magic Spell Class!\\nTell me what you want to achieve, and Iâ€™ll suggest the perfect spell for it!\\nIf you ask in the format: 'What spell can I use when I ~?', I can give you even better suggestions!\"\n",
    "    txt_box = gr.Textbox(value=\"What spell can I use when I \", interactive=True)\n",
    "    prof_IMG = show_image(\"./IMG/Spell_stand.jpg\")\n",
    "    chat_history.append([None,bot_message])\n",
    "    return chat_history, prof_IMG, txt_box, enable_btn, enable_btn, enable_btn\n",
    "\n",
    "def potion_chatting(chat_history, request: gr.Request):\n",
    "    global spell_chat, potion_chat, other_chat, house_chat\n",
    "    chat_history.clear()\n",
    "    print(f\"chatting ip: {request.client.host}\")\n",
    "    spell_chat, potion_chat, other_chat, house_chat = False, True, False, False\n",
    "    bot_message = \"Welcome to Potion Class!\\nTell me what you want to achieve, and Iâ€™ll suggest the perfect potion for it!\\nIf you ask in the format: 'What potion can I make when I ~?', I can give you even better suggestions!\"\n",
    "    txt_box = gr.Textbox(value=\"What potion can I make when I \", interactive=True)\n",
    "    prof_IMG = show_image(\"./IMG/Potion_stand.jpg\")\n",
    "    chat_history.append([None,bot_message])\n",
    "    return chat_history, prof_IMG, txt_box, enable_btn, enable_btn, enable_btn\n",
    "\n",
    "def other_chatting(chat_history, request: gr.Request):\n",
    "    global spell_chat, potion_chat, other_chat, house_chat\n",
    "    chat_history.clear()\n",
    "    print(f\"chatting ip: {request.client.host}\")\n",
    "    spell_chat, potion_chat, other_chat, house_chat = False, False, True, False\n",
    "    bot_message = \"Welcome to Library!\\nWelcome to the library! Feel free to ask me anything if you're curious!\"\n",
    "    txt_box = gr.Textbox(placeholder=\"Ask anything you're curious about in the Wizarding World!\", value=\"\", interactive=True)\n",
    "    prof_IMG = show_image(\"./IMG/Other_stand.jpg\")\n",
    "    chat_history.append([None,bot_message])\n",
    "    return chat_history, prof_IMG, txt_box, enable_btn, enable_btn, enable_btn\n",
    "\n",
    "def house_chatting(chat_history, request: gr.Request):\n",
    "    global spell_chat, potion_chat, other_chat, house_chat\n",
    "    chat_history.clear()\n",
    "    print(f\"chatting ip: {request.client.host}\")\n",
    "    spell_chat, potion_chat, other_chat, house_chat = False, False, False, True\n",
    "    bot_message = \"Hmm, let's seeâ€¦ where shall I place you?\"\n",
    "    txt_box = gr.Textbox(placeholder=\"faiosdjfopasdjfoasdjfopsd\", value=\"\", interactive=True)\n",
    "    prof_IMG = show_image(\"./IMG/sorting_hat.jpg\")\n",
    "    chat_history.append([None,bot_message])\n",
    "    return chat_history, prof_IMG, txt_box, enable_btn, enable_btn, enable_btn\n",
    "\n",
    "def ask_question(chat_history, text_data, request: gr.Request):\n",
    "    global spell_chat, potion_chat, other_chat, house_chat\n",
    "    print(spell_chat, potion_chat, other_chat, house_chat)\n",
    "    if spell_chat:\n",
    "        print(text_data)\n",
    "        prof_IMG = show_image(\"./IMG/Spell_think.jpg\")\n",
    "        chat_history.append([text_data, None])\n",
    "        text_data = gr.Textbox(value=\"What spell can I use when I \", interactive=True)\n",
    "        return chat_history, prof_IMG, text_data\n",
    "    \n",
    "    if potion_chat:\n",
    "        print(text_data)\n",
    "        prof_IMG = show_image(\"./IMG/Potion_think.jpg\")\n",
    "        chat_history.append([text_data, None])\n",
    "        text_data = gr.Textbox(placeholder=\"Ask anything you're curious about in the Wizarding World!\", value=\"What potion can I make when I \", interactive=True)\n",
    "        return chat_history, prof_IMG, text_data\n",
    "    \n",
    "    if other_chat:\n",
    "        print(text_data)\n",
    "        prof_IMG = show_image(\"./IMG/Other_think.jpg\")\n",
    "        chat_history.append([\"test message\", None])\n",
    "        text_data = gr.Textbox(placeholder=\"faiosdjfopasdjfoasdjfopsd\", value=\"\", interactive=True)\n",
    "        return chat_history, prof_IMG, text_data\n",
    "    \n",
    "    if house_chat:\n",
    "        print(text_data)\n",
    "        prof_IMG = show_image(\"./IMG/sorting_hat.jpg\")\n",
    "        chat_history.append([\"test message\", None])\n",
    "        text_data = gr.Textbox(value=\"asdfasdfasdfasdf\", interactive=True)\n",
    "        return chat_history, prof_IMG, text_data\n",
    "\n",
    "background_image_path = Path(\"./IMG/bg1.jpg\").resolve()\n",
    "\n",
    "\n",
    "\n",
    "custom_css = f\"\"\"\n",
    "\n",
    "body {{\n",
    "    background-image: url('file://{background_image_path}');\n",
    "    background-size: cover;  /* ë°°ê²½ ì´ë¯¸ì§€ê°€ í™”ë©´ ì „ì²´ë¥¼ ë®ë„ë¡ ì„¤ì • */\n",
    "    background-repeat: no-repeat;  /* ì´ë¯¸ì§€ ë°˜ë³µ ë°©ì§€ */\n",
    "    background-position: center;  /* ì´ë¯¸ì§€ ê°€ìš´ë° ì •ë ¬ */\n",
    "}}\n",
    "\"\"\"\n",
    "custom_css = f\"\"\"\n",
    "body {{\n",
    "    background-image: url('file://{background_image_path}');\n",
    "    background-size: cover;  /* Make the image cover the entire background */\n",
    "    background-position: center;  /* Center the background image */\n",
    "    background-repeat: no-repeat;  /* Do not repeat the background image */\n",
    "}}\n",
    "\n",
    ".dark body {{\n",
    "    background-image: url('file://{background_image_path}');\n",
    "    background-size: cover;\n",
    "    background-position: center;\n",
    "    background-repeat: no-repeat;\n",
    "    background-color: #333;  /* Optional: Fallback color for dark mode */\n",
    "}}\n",
    "\n",
    ".gradio-container {{\n",
    "    background: transparent !important;  /* Make container background transparent to see the full background image */\n",
    "}}\n",
    "\"\"\"\n",
    "def build_gradio(concurrency_count=10):\n",
    "    textbox = gr.Textbox(show_label=False, placeholder=\"Welcome to Hogwart!\", container=False, interactive = False)\n",
    "    with gr.Blocks(\n",
    "            css=custom_css,\n",
    "            theme='abidlabs/pakistan',\n",
    "            \n",
    "            \n",
    "        ) as demo:\n",
    "        state = gr.State()\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=3):\n",
    "\n",
    "                # imagebox = gr.Image(interactive=False)\n",
    "                imagebox = gr.Image(value=show_image(\"./IMG/HogwartGemma.jpg\"), type=\"pil\", interactive=False, show_label=False, show_download_button=False, show_fullscreen_button=False)\n",
    "                # image_process_mode = gr.Radio(\n",
    "                #     [\"Crop\", \"Resize\", \"Pad\", \"Default\"],\n",
    "                #     value=\"Default\",\n",
    "                #     label=\"Preprocess for non-square image\", visible=False)\n",
    "                \n",
    "                spell_btn = gr.Button(value=\"ğŸª„ Spell\", interactive = True, scale = 2)\n",
    "                potion_btn = gr.Button(value=\"ğŸ§‰ Potion\", interactive = True, scale = 2)\n",
    "                other_btn = gr.Button(value=\"ğŸ“š Others\", interactive = True, scale = 2)\n",
    "                house_btn = gr.Button(value=\"ğŸŒŸ Find your House\", interactive = True, scale = 2)\n",
    "\n",
    "            with gr.Column(scale=8):\n",
    "                chatbot = gr.Chatbot(\n",
    "                    label='Free to ask!',\n",
    "                    height=600,\n",
    "                )\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=8):\n",
    "                        textbox.render()\n",
    "                    with gr.Column(scale=1, min_width=50):\n",
    "                        submit_btn = gr.Button(value=\"Send\", variant=\"primary\", interactive = False)\n",
    "\n",
    "                with gr.Row(elem_id=\"buttons\") as button_row:\n",
    "                    finish_btn = gr.Button(value=\"ğŸ End Talking\", interactive = False)\n",
    "                    clear_btn = gr.Button(value=\"ğŸ—‘ï¸  Clear\", interactive=False)    \n",
    "        box_list = [spell_btn, potion_btn, other_btn, house_btn, textbox, submit_btn, finish_btn, clear_btn]\n",
    "        spell_btn.click(\n",
    "            spell_chatting,\n",
    "            inputs=[chatbot],\n",
    "            outputs=[chatbot, imagebox, textbox, submit_btn, finish_btn, clear_btn]\n",
    "        )\n",
    "        potion_btn.click(\n",
    "            potion_chatting,\n",
    "            inputs=[chatbot],\n",
    "            outputs=[chatbot, imagebox, textbox, submit_btn, finish_btn, clear_btn]\n",
    "        )\n",
    "        other_btn.click(\n",
    "            other_chatting,\n",
    "            inputs=[chatbot],\n",
    "            outputs=[chatbot, imagebox, textbox, submit_btn, finish_btn, clear_btn]\n",
    "        )\n",
    "        house_btn.click(\n",
    "            house_chatting,\n",
    "            inputs=[chatbot],\n",
    "            outputs=[chatbot, imagebox, textbox, submit_btn, finish_btn, clear_btn]\n",
    "        )\n",
    "        submit_btn.click(\n",
    "            ask_question,\n",
    "            inputs = [chatbot, textbox],\n",
    "            outputs = [chatbot, imagebox, textbox]\n",
    "        )\n",
    "        finish_btn.click()\n",
    "        clear_btn.click()\n",
    "\n",
    "    return demo\n",
    "\n",
    "import argparse\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # ì»¨íŠ¸ë¡¤ëŸ¬ì—ì„œ ì‚¬ìš©ê°€ëŠ¥í•œ ëª¨ë¸ ê°€ì ¸ì˜´. ë‚˜ëŠ” ëª¨ë¸ í•˜ë‚˜ë§Œ ì“¸ ê²ƒì´ê¸° ë•Œë¬¸ì—, get_model_list()ì—ì„œ í•´ë‹¹ ëª¨ë¸ì´ ë™ì‘í•˜ê³  ìˆëŠ” urlì„ ë„˜ê²¨ì£¼ë©´ ëœë‹¤!\n",
    "    # models = [args.model_url] \n",
    "    \n",
    "    # HP_chatbot = HPChatBot(load_in_8bit=True,\n",
    "    #                    bnb_8bit_compute_dtype=torch.float16,\n",
    "    #                    bnb_8bit_use_double_quant=True,\n",
    "    #                    bnb_8bit_quant_type='nf8')\n",
    "    \n",
    "    # Gradioë¥¼ ì´ìš©í•´ì„œ ë°ëª¨ ë§Œë“¤ê¸°\n",
    "    demo = build_gradio()\n",
    "    demo.queue(\n",
    "        api_open=False\n",
    "    ).launch(\n",
    "        server_port=7860,\n",
    "        share=True\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
